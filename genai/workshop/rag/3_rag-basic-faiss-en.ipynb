{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RAG (Retrieval-Augmented Generation) Hands-on Lab\n",
    "\n",
    "### RAG 개요\n",
    "\n",
    "RAG는 언어 모델의 성능을 개선하는 간단하면서도 유용한 기법으로 두 단계 프로세스로 이뤄집니다.\n",
    "\n",
    "첫 번째 단계로 사용자가 입력한 프롬프트를 임베딩하여 지식 소스에서 관련 문서를 검색하는데, 이는 네이버나 구글 검색에서 관련 검색 결과를 가져오는 방식과 같습니다.\n",
    "임베딩에 특화된 모델을 사용하거나 대규모 언어 모델을 임베딩 모델로 사용할 수 있죠. 지식 소스는 인메모리 DB로 FAISS를 사용하거나 ChromaDB와 같은 벡터 DB, 아니면 OpenSearch를 적용할 수 있습니다.\n",
    "\n",
    "두 번째 단계에서는 검색 결과를 같이 프롬프트에 포함하여 LLM에 유입함으로써 최종 응답 결과를 생성합니다. LLM의 답변 범위를 검색 결과로 제한함으로써 모델 환각(hallucination) 현상을 완화합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Prepare Large Language Model (LLM) and Embedding Model \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('../templates') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESTAPI_ID =  a6uibyuabj\n",
      "API GATEWAY URL =  https://a6uibyuabj.execute-api.us-west-2.amazonaws.com/api/\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sagemaker, boto3, json\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "from typing import Any, Dict, List, Optional\n",
    "from ssm import parameter_store\n",
    "from termcolor import colored\n",
    "from common import get_apigateway_url\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "\n",
    "RESTAPI_ID, URL = get_apigateway_url()\n",
    "print(\"RESTAPI_ID = \", RESTAPI_ID)\n",
    "print(\"API GATEWAY URL = \", URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_NAME: FALCON-40B\n",
      "LLM_URL: https://a6uibyuabj.execute-api.us-west-2.amazonaws.com/api/llm/falcon_40b\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"FALCON-40B\"\n",
    "\n",
    "LLM_INFO = {\n",
    "    \"LLAMA2-7B\": f\"{URL}llm/llama2_7b\",\n",
    "    \"FALCON-40B\": f\"{URL}llm/falcon_40b\",    \n",
    "    \"KULLM-12-8B\": f\"{URL}llm/kkulm_12_8b\",\n",
    "}\n",
    "\n",
    "LLM_URL = LLM_INFO[MODEL_NAME]\n",
    "EMB_URL = f\"{URL}/emb/gptj_6b\"\n",
    "\n",
    "HEADERS = {    \n",
    "    'Content-Type': 'application/json',\n",
    "    'Accept': 'application/json',\n",
    "}\n",
    "\n",
    "if 'falcon_40b' in LLM_URL:\n",
    "    LLM_RESPONSE_KEY = \"generated_text\"\n",
    "else:\n",
    "    LLM_RESPONSE_KEY = \"generation\"\n",
    "    \n",
    "print (f'MODEL_NAME: {MODEL_NAME}\\nLLM_URL: {LLM_URL}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"LLAMA2-7B\": {\n",
    "        'max_new_tokens': 128,\n",
    "        'top_p': 0.9,\n",
    "        'temperature': 0.1,\n",
    "        'return_full_text': False\n",
    "    },   \n",
    "    \"FALCON-40B\": {\n",
    "        \"max_new_tokens\": 200,\n",
    "        \"max_length\": 256,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.4,\n",
    "        \"return_full_text\": False,\n",
    "        \"include_prompt_in_result\": False\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Step 2. Ask a question to LLM without RAG\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lib_en import Llama2ContentHandlerAmazonAPIGateway, FalconContentHandlerAmazonAPIGateway\n",
    "from langchain.llms import AmazonAPIGateway\n",
    "\n",
    "llm = AmazonAPIGateway(api_url=LLM_URL, headers=HEADERS)\n",
    "if MODEL_NAME == \"FALCON-40B\": llm.content_handler = FalconContentHandlerAmazonAPIGateway()\n",
    "elif MODEL_NAME in [\"LLAMA2-7B\", \"LLAMA2-13B\"]: llm.content_handler = Llama2ContentHandlerAmazonAPIGateway()\n",
    "params = PARAMS[MODEL_NAME]\n",
    "llm.model_kwargs = params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without providing the context\n",
    "- 컨텍스트 없이 질의응답 수행 (모델 환각 확인) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mWhich instances can I use with Managed Spot Training in Amazon SageMaker? Please provide answer within 50 words.\u001b[0m\n",
      "\n",
      "Managed Spot Training in Amazon SageMaker can be used with the following instances: \n",
      "- Amazon SageMaker p3 instances \n",
      "- Amazon SageMaker p2 instances \n",
      "- Amazon SageMaker p3dn instances \n",
      "- Amazon SageMaker p2dn instances \n",
      "- Amazon SageMaker p3.2xlarge instances \n",
      "- Amazon SageMaker p3.8xlarge instances \n",
      "- Amazon SageMaker p3.16xlarge instances \n",
      "- Amazon SageMaker p2.xlarge instances \n",
      "- Amazon SageMaker p2.8xlarge instances \n",
      "- Amazon SageMaker p2.16xlarge instances \n",
      "- Amazon SageMaker p2.xlarge instances with NVIDIA GPUs \n",
      "- Amazon SageMaker p2.8xlarge instances with NVIDIA GPUs \n",
      "- Amazon SageMaker p2.16xlarge instances with NVIDIA GPUs \n",
      "- Amazon SageMaker p3.2xlarge instances with NVIDIA GPUs \n",
      "- Amazon SageMaker p3.8xlarge instances with NVIDIA GPUs \n",
      "- Amazon SageMaker\n"
     ]
    }
   ],
   "source": [
    "question = \"Which instances can I use with Managed Spot Training in Amazon SageMaker? Please provide answer within 50 words.\"\n",
    "\n",
    "payload = {\n",
    "    'inputs': question,\n",
    "    'parameters': params\n",
    "}\n",
    "\n",
    "print(colored(question, 'green'))\n",
    "response = requests.post(url=LLM_URL, headers=HEADERS, json=payload)\n",
    "print(response.json()[0][LLM_RESPONSE_KEY])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Context\n",
    "- 추가 컨텍스트 or few-shot 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAnswer based on context:\n",
      "\n",
      "Managed Spot Training can be used with all instances supported in Amazon SageMaker. \n",
      "Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n",
      "\n",
      "Which instances can I use with Managed Spot Training in Amazon SageMaker? Please provide answer within 50 words.\u001b[0m\n",
      "\n",
      "As an AI language model, I do not have access to the current availability of instances in Amazon SageMaker. However, Managed Spot Training is supported on all instance types that are currently available in Amazon SageMaker. This includes both GPU and CPU instances.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"Managed Spot Training can be used with all instances supported in Amazon SageMaker. \n",
    "Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\"\"\"\n",
    "    \n",
    "prompt = \"\"\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\"\"\n",
    "\n",
    "text_input = prompt.replace(\"{context}\", context)\n",
    "text_input = text_input.replace(\"{question}\", question)\n",
    "\n",
    "payload = {\n",
    "    'inputs': text_input,\n",
    "    'parameters': params\n",
    "}\n",
    "\n",
    "print(colored(text_input, 'green'))\n",
    "\n",
    "response = requests.post(url=LLM_URL, headers=HEADERS, json=payload)\n",
    "print(response.json()[0][LLM_RESPONSE_KEY])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As an AI language model, I don't have access to the current list of instances supported by Amazon SageMaker. However, based on the official documentation, Managed Spot Training can be used with all instances supported in Amazon SageMaker. It is recommended to check the SageMaker documentation for the latest list of supported instances.\n"
     ]
    }
   ],
   "source": [
    "result = llm(text_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Step 3. Use RAG based approach with [LangChain](https://python.langchain.com/en/latest/index.html) \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Loaders\n",
    "<img src=\"../images/RAG-Page-3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document loader를 사용하여 원본 소스에서 데이터를 문서로 로드합니다. 문서는 텍스트와 관련 메타데이터를 의미합니다. 예를 들어 간단한 텍스트 파일을 로드하거나 웹페이지의 텍스트 콘텐츠를 로드하거나 YouTube 동영상의 스크립트를 로드하기 위한 Document loader가 있습니다. Document loader는  기본적으로 'load' 메서드를 사용하며, 상황에 따라 'lazy load'도 사용할 수 있습니다.\n",
    "\n",
    "pdf, html, json, txt, csv와 같은 다양한 파일 유형에 사용할 수 있는 다양한 'loader'는 물론 Slack, Twitter 등과 같은 타사 플랫폼과의 통합도 지원합니다. 전체 목록은 여기에서 확인해 주세요. https://python.langchain.com/docs/modules/data_connection/document_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import Chroma, AtlasDB, FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 예제 데이터를 다운로드합니다. Amazon SageMaker FAQ (https://aws.amazon.com/sagemaker/faqs/) 를 지식 라이브러리로 사용하겠습니다. 데이터는 질문과 답변의 두 열이 있는 CSV 파일로 구성되며, 이 중에서 답변 열만 지식 라이브러리의 문서로 사용하여 쿼리 기반으로 관련 문서를 검색합니다.\n",
    "\n",
    "**필요에 따라 예제 데이터 세트를 여러분의 QnA 데이터 세트로 대체하여 구축할 수 있습니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_folder = \"../dataset\"\n",
    "save_dataset_path = f\"{dataset_folder}/processed/processed_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "all_files = glob.glob(os.path.join(f\"{dataset_folder}/raw/\", \"*_FAQs*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_knowledge = pd.concat(\n",
    "    (pd.read_csv(f, header=None, names=[\"Question\", \"Answer\"]) for f in all_files),\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Amazon SageMaker?</td>\n",
       "      <td>Amazon SageMaker is a fully managed service to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In which Regions is Amazon SageMaker available...</td>\n",
       "      <td>For a list of the supported Amazon SageMaker A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the service availability of Amazon Sag...</td>\n",
       "      <td>Amazon SageMaker is designed for high availabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does Amazon SageMaker secure my code?</td>\n",
       "      <td>Amazon SageMaker stores code in ML storage vol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What security measures does Amazon SageMaker h...</td>\n",
       "      <td>Amazon SageMaker ensures that ML model artifac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>What are Amazon SageMaker Savings Plans?</td>\n",
       "      <td>Amazon SageMaker Savings Plans offer a flexibl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Why should I use Amazon SageMaker Savings Plans?</td>\n",
       "      <td>If you have a consistent amount of Amazon Sage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>How can I get started with Amazon SageMaker Sa...</td>\n",
       "      <td>You can get started with Savings Plans from AW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>How are Savings Plans for Amazon SageMaker dif...</td>\n",
       "      <td>The difference between Savings Plans for Amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>How do Savings Plans work with AWS Organizatio...</td>\n",
       "      <td>Savings Plans can be purchased in any account ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Question  \\\n",
       "0                            What is Amazon SageMaker?   \n",
       "1    In which Regions is Amazon SageMaker available...   \n",
       "2    What is the service availability of Amazon Sag...   \n",
       "3            How does Amazon SageMaker secure my code?   \n",
       "4    What security measures does Amazon SageMaker h...   \n",
       "..                                                 ...   \n",
       "149           What are Amazon SageMaker Savings Plans?   \n",
       "150   Why should I use Amazon SageMaker Savings Plans?   \n",
       "151  How can I get started with Amazon SageMaker Sa...   \n",
       "152  How are Savings Plans for Amazon SageMaker dif...   \n",
       "153  How do Savings Plans work with AWS Organizatio...   \n",
       "\n",
       "                                                Answer  \n",
       "0    Amazon SageMaker is a fully managed service to...  \n",
       "1    For a list of the supported Amazon SageMaker A...  \n",
       "2    Amazon SageMaker is designed for high availabi...  \n",
       "3    Amazon SageMaker stores code in ML storage vol...  \n",
       "4    Amazon SageMaker ensures that ML model artifac...  \n",
       "..                                                 ...  \n",
       "149  Amazon SageMaker Savings Plans offer a flexibl...  \n",
       "150  If you have a consistent amount of Amazon Sage...  \n",
       "151  You can get started with Savings Plans from AW...  \n",
       "152  The difference between Savings Plans for Amazo...  \n",
       "153  Savings Plans can be purchased in any account ...  \n",
       "\n",
       "[154 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker is a fully managed service to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For a list of the supported Amazon SageMaker A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon SageMaker is designed for high availabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon SageMaker stores code in ML storage vol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon SageMaker ensures that ML model artifac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Answer\n",
       "0  Amazon SageMaker is a fully managed service to...\n",
       "1  For a list of the supported Amazon SageMaker A...\n",
       "2  Amazon SageMaker is designed for high availabi...\n",
       "3  Amazon SageMaker stores code in ML storage vol...\n",
       "4  Amazon SageMaker ensures that ML model artifac..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop the question column as we're not using it for the exercise.\n",
    "df_knowledge.drop([\"Question\"], axis=1, inplace=True)\n",
    "\n",
    "#saving the modified df \n",
    "df_knowledge.to_csv(save_dataset_path, header=False, index=False)\n",
    "\n",
    "df_knowledge.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_loader = CSVLoader(file_path=save_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== content ===\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: For a list of the supported Amazon SageMaker AWS Regions, please visit the AWS Regional Services page. Also, for more information, see Regional endpoints in the AWS general reference guide.\n",
      "=== metadata ===\n",
      "{'source': '../dataset/processed/processed_data.csv', 'row': 0}\n",
      "=== chunks ===\n",
      "[Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 0}), Document(page_content='learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: For', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 0}), Document(page_content='a list of the supported Amazon SageMaker AWS Regions, please visit the\\xa0AWS Regional Services page.', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 0}), Document(page_content='Also, for more information, see\\xa0Regional endpoints\\xa0in the AWS general reference guide.', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 0})]\n"
     ]
    }
   ],
   "source": [
    "documents = csv_loader.load()\n",
    "\n",
    "for document in documents:\n",
    "    content = document.page_content\n",
    "    metadata = document.metadata\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents([document])\n",
    "    \n",
    "    print(f\"=== content ===\\n{content}\")\n",
    "    print(f\"=== metadata ===\\n{metadata}\")\n",
    "    print(f\"=== chunks ===\\n{chunks}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.013422408141195774, -0.003006801474839449, -0.0014634798280894756, -0.013446807861328125, -0.01952643319964409]\n",
      "[0.013422408141195774, -0.003006801474839449, -0.0014634798280894756, -0.013446807861328125, -0.01952643319964409]\n"
     ]
    }
   ],
   "source": [
    "from lib_en import EmbeddingAmazonApiGateway\n",
    "emb = EmbeddingAmazonApiGateway(api_url=EMB_URL)\n",
    "\n",
    "prompt = \"What is Amazon SageMaker's advantages for Data Scientists? Please summarize in 100 words\"\n",
    "\n",
    "result = emb.embed_query(prompt)\n",
    "print(result[0:5])\n",
    "\n",
    "emb_results = emb.embed_documents([prompt])\n",
    "print(emb_results[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result == emb_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the VectorstoreIndex\n",
    "\n",
    "RAG는 `VectorstoreIndexCreator` 로 쉽고 빠르게 구현할 수 있습니다. 다만, 프롬프트 커스터마이징 및 세부 파라메터 설정이 필요하거나 보다 세밀한 디버깅 시에는 아래 절 (Step 4.)의 과정을 거치는 것을 권장합니다.\n",
    "- FAISS: https://github.com/facebookresearch/faiss\n",
    "- LangChain document: https://python.langchain.com/docs/modules/data_connection/vectorstores/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=0, separators=[\" \", \",\", \".\", \"\\n\"])\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS, # use FAISS as the vectorestore to index and search embeddings\n",
    "    embedding=emb,\n",
    "    text_splitter=text_splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 871 ms, sys: 37.3 ms, total: 909 ms\n",
      "Wall time: 43.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "index = index_creator.from_loaders([csv_loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mWhich instances can I use with Managed Spot Training in Amazon SageMaker? Please provide answer within 50 words.\u001b[0m\n",
      " Managed Spot Training in Amazon SageMaker can be used with any instance type that supports Spot instances, including EC2 instances, Amazon EC2 Spot instances, and Amazon EC2 Spot Fleet. Spot instances are available at up to a 90% discount compared to On-Demand instances, making them an affordable option for training machine learning models.\n"
     ]
    }
   ],
   "source": [
    "answer = index.query(question=question, llm=llm)\n",
    "print(colored(question, 'green'))\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Step 4. Customize the QA application above with different prompt\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Use the vectorstore index as a retriever within a RetrievalQA chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예시처럼 RAG를 매우 편리하고 빠르게 구현할 수 있지만, `VectorstoreIndex`는 \"블랙박스\"처럼 사용 중인 프롬프트를 완전히 제어할 수 있는 옵션이 제공되지 않습니다. \n",
    "\n",
    "이 경우에는 index를 \"retriever(검색기)\"로 래핑하고 사용자 지정 프롬프트 템플릿을 활용하는 RetrievalQA 객체와 vectorstore index를 retriever 객체로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorstore Retriever Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='that you use, not for individual items. You can read more about this in our metering guide. In addition to the notebooks, you can also start and run terminals and interactive shells in SageMaker Studio, all on the same compute instance. Each application runs within a container or image. SageMaker', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 64}), Document(page_content='your Managed Spot Training jobs from restarting if capacity is pre-empted. When you enable checkpoints, Amazon SageMaker resumes your Managed Spot Training jobs from the last checkpoint.', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 87}), Document(page_content='set of solutions for the most common use cases that can be deployed readily with just a few clicks. The solutions are fully customizable and showcase the use of AWS CloudFormation templates and reference architectures so you can accelerate your ML journey. SageMaker JumpStart also supports', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 23}), Document(page_content='any separate AWS services you use within SageMaker Pipelines.', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 35})]\n"
     ]
    }
   ],
   "source": [
    "retriever = index.vectorstore.as_retriever()\n",
    "print(retriever.get_relevant_documents(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retriever 래핑 시에는 Similarity Search가 디폴트로 적용되지만, MMR(Max Marginal Relevance)를 적용할 수도 있습니다. search_kwargs argument 또한 선택적으로 입력할 수 있으며 주요 파라메터는 아래와 같습니다.\n",
    "\n",
    "- `k`: top_k의 문서 개수로 기본값은 4입니다.\n",
    "- `score_threshold`: \"similarity_score_threshold\" 검색 유형을 사용하는 경우 검색기가 반환하는 문서의 최소 관련성을 설정할 수 있습니다.\n",
    "- `fetch_k`: MMR 알고리즘에 전달할 문서 개수로 기본값은 20입니다.\n",
    "- `lambda_mult`: MMR 알고리즘이 반환하는 결과의 다양성을 제어하며, 1은 최소 다양성, 0은 최대 다양성입니다. 기본값은 0.5입니다.\n",
    "- `filter`: 문서의 메타데이터를 기반으로 검색할 문서에 대한 필터를 정의할 수 있습니다. 벡터스토어에 메타데이터가 저장되어 있지 않은 경우에는 이 옵션이 적용되지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='that you use, not for individual items. You can read more about this in our metering guide. In addition to the notebooks, you can also start and run terminals and interactive shells in SageMaker Studio, all on the same compute instance. Each application runs within a container or image. SageMaker', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 64}), Document(page_content='any separate AWS services you use within SageMaker Pipelines.', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 35}), Document(page_content='your Managed Spot Training jobs from restarting if capacity is pre-empted. When you enable checkpoints, Amazon SageMaker resumes your Managed Spot Training jobs from the last checkpoint.', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 87})]\n"
     ]
    }
   ],
   "source": [
    "retriever = index.vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3, \"fetch_k\": 10})\n",
    "print(retriever.get_relevant_documents(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize your own prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "###\n",
    "{context}\n",
    "###\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retriever wrapping w/ Chains\n",
    "\n",
    "LangChain은 컨텍스트를 요약하기 위한 다양한 LLM 체인을 제공하며 대표적으로 4가지 방법(Stuff, Refine, Map reduce, Map re-rank)을 지원합니다.\n",
    "\n",
    "- **Stuff**: 가장 기본적인 방법으로 프롬프트에 모든 관련 데이터를 컨텍스트로 포함시켜 LLM에 전달합니다. 가장 간단한 접근 방식으로 청크 크기가 작고 검색 결과가 많지 않을 때 효과적입니다. 하지만 LLM에는 한 번의 호출로 처리할 수 있는 토큰의 최대 개수인 컨텍스트 길이(context length)가 존재합니다. LLM의 컨텍스트 길이보다 긴 텍스트를 처리할 때에는 청크 크기를 줄이거나 MapReduce나 Refine 같은 다른 방법을 사용해야 합니다.\n",
    "<img src=\"https://python.langchain.com/assets/images/stuff-818da4c66ee17911bc8861c089316579.jpg\"/>\n",
    "- **Refine**: 청크된 문서 리스트를 순회하면서 이전 문서의 LLM 중간 답변 결과를 LLM 체인에 컨텍스트로 전달하여 LLM 답변을 개선합니다. \n",
    "<img src=\"https://python.langchain.com/assets/images/refine-a70f30dd7ada6fe5e3fcc40dd70de037.jpg\"/>\n",
    "- **Map reduce**: 개별 데이터 청크에 대한 초기 프롬프트의 힘을 활용하여 문서의 특정 섹션만을 기반으로 요약 또는 답변을 생성합니다. (Map) 그 이후초기 출력 결과를 결합하는 별도의 프롬프트를 사용하여 전체 문서에 걸친 포괄적이고 일관된 요약 또는 답변을 생성합니다. (Reduce) \n",
    "<img src=\"https://python.langchain.com/assets/images/map_reduce-c65525a871b62f5cacef431625c4d133.jpg\"/>\n",
    "- **Map re-rank**: Map reduce와 비슷하지만, 답변이 얼마나 확실한지에 대한 점수를 같이 부여합니다. 최종적으로 가장 높은 점수를 받은 답변이 반환됩니다.\n",
    "<img src=\"https://python.langchain.com/assets/images/map_rerank-0302b59b690c680ad6099b7bfe6d9fe5.jpg\"/>\n",
    "보다 자세한 내용은 https://python.langchain.com/docs/modules/chains/document/ 을 참조하기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, chain_type_kwargs=chain_type_kwargs,\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32mWhich instances can I use with Managed Spot Training in Amazon SageMaker? Please provide answer within 50 words.\u001b[0m\n",
      " You can use any instance type that is available in the AWS Spot Market, including the latest generation of instances such as the EC2 C6g, M6g, R6g, and T4g instances.\n"
     ]
    }
   ],
   "source": [
    "results = qa(question)\n",
    "answer = results[\"result\"]\n",
    "#answer = qa.run(question)\n",
    "print(colored(question, 'green'))\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another approach: Step-by-step RAG\n",
    "\n",
    "좀 더 나아가 위의 `VectorstoreIndexCreator`를 분해하여 내부에서 어떤 일이 일어나는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: For a list of the supported Amazon SageMaker AWS Regions, please visit the\\xa0AWS Regional Services page. Also, for more information, see\\xa0Regional endpoints\\xa0in the AWS general reference guide.', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 0}), Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazon’s proven, high-availability data centers, with service stack replication configured across three facilities in each AWS Region to provide fault tolerance in the event of a server failure or Availability Zone outage.', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 1})]\n"
     ]
    }
   ],
   "source": [
    "#using the same loader\n",
    "documents = csv_loader.load()\n",
    "\n",
    "#looking into the first docs\n",
    "print(documents[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RecursiveCharacterTextSplitter`로 각 문서를 청킹하고 청킹한 문서를 `.from_documents`로 임베딩한 다음, 임베딩 결과를 벡터 저장소에 저장하고 관련 문서를 색인합니다. 본 예시에서는 FAISS를 사용하지만, 유스케이스에 따라 ChromaDB, OpenSearch 등의 다양한 벡터 저장소 라이브러리나 서비스를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 830 ms, sys: 42.7 ms, total: 872 ms\n",
      "Wall time: 41.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=5)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# generate embeddings and load that into FAISS\n",
    "vectorstore = FAISS.from_documents(texts, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용자 쿼리를 기반으로 가장 관련성이 높은 상위 k개의 문서를 식별합니다. (예: k = 3) LLM의 토큰 길이가 제안되어 있기에, 상위 k개의 문서만 LLM에 전달함으로써 컨텍스트 길이를 제어해야 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mWhich instances can I use with Managed Spot Training in Amazon SageMaker? Please provide answer within 50 words.\u001b[0m\n",
      "[Document(page_content='that you use, not for individual items. You can read more about this in our metering guide. In addition to the notebooks, you can also start and run terminals and interactive shells in SageMaker Studio, all on the same compute instance. Each application runs within a container or image. SageMaker', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 64}), Document(page_content='your Managed Spot Training jobs from restarting if capacity is pre-empted. When you enable checkpoints, Amazon SageMaker resumes your Managed Spot Training jobs from the last checkpoint.', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 87}), Document(page_content='and endpoints, based on\\xa0SageMaker pricing. There is no additional charge for using SageMaker JumpStart.', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 28})]\n"
     ]
    }
   ],
   "source": [
    "docs = vectorstore.similarity_search(question, k=3)\n",
    "print(colored(question, 'green'))\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "반드시 retriever로 VectorDB를 사용할 필요가 없으며, 다른 retriever를 사용할 수 있습니다. 아래 코드 스니펫을 참조해 주세요.\n",
    "- https://python.langchain.com/docs/modules/data_connection/retrievers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import SVMRetriever\n",
    "\n",
    "svm_retriever = SVMRetriever.from_documents(texts, emb)\n",
    "docs_svm = svm_retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 검색된 문서를 프롬프트 및 질문과 결합하여 LLM에 입력하여 추론을 수행합니다. 모델 환각 현상이 개선되는 것을 확인하기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "###\n",
    "{context}\n",
    "###\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain = load_qa_chain(llm=llm, chain_type=\"stuff\", prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[Document(page_content='that you use, not for individual items. You can read more about this in our metering guide. In addition to the notebooks, you can also start and run terminals and interactive shells in SageMaker Studio, all on the same compute instance. Each application runs within a container or image. SageMaker', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 64}), Document(page_content='your Managed Spot Training jobs from restarting if capacity is pre-empted. When you enable checkpoints, Amazon SageMaker resumes your Managed Spot Training jobs from the last checkpoint.', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 87}), Document(page_content='and endpoints, based on\\xa0SageMaker pricing. There is no additional charge for using SageMaker JumpStart.', metadata={'source': '../dataset/processed/processed_data.csv', 'row': 28})]\u001b[0m\n",
      " Managed Spot Training in Amazon SageMaker can be used with the following instances: \n",
      "- ml.t2.medium\n",
      "- ml.t2.large\n",
      "- ml.m5.xlarge\n",
      "- ml.m5.2xlarge\n",
      "- ml.m5.4xlarge\n",
      "- ml.m5.8xlarge\n",
      "- ml.m5.16xlarge\n",
      "- ml.p3.2xlarge\n",
      "- ml.p3.8xlarge\n",
      "- ml.p3.16xlarge\n",
      "- ml.p3.2xlarge\n",
      "- ml.p3.8xlarge\n",
      "- ml.p3.16xlarge\n",
      "- ml.p3.24xlarge\n",
      "- ml.p3.2xlarge\n",
      "- ml.p3.8xlarge\n",
      "- ml.p3.16xlarge\n",
      "- ml.p3.24xlarge\n",
      "- ml.p3.2xlarge\n"
     ]
    }
   ],
   "source": [
    "result = chain({\"input_documents\": docs, \"question\": question})\n",
    "print(colored(result['input_documents'], 'green'))\n",
    "print(result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationalRetrievalChain\n",
    "\n",
    "질의응답/채팅 히스토리를 피드백으로 저장하고 그 피드백을 기반으로 이후 대화를 이어나가게 수행이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "#retriever = index.vectorstore.as_retriever()\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "SageMaker Data Wrangler, Amazon SageMaker Feature Store, Amazon SageMaker Experiments, Amazon SageMaker Debugger, and Amazon SageMaker Model Monitor, can be added to SageMaker Pipelines.\n",
      "\n",
      "SageMaker Neo documentation.\n",
      "\n",
      "get started with SageMaker Inference Recommender in minutes while selecting an instance type and get recommendations for optimal endpoint configurations within hours, eliminating weeks of manual testing and tuning time. With SageMaker Inference Recommender, you pay only for the SageMaker ML\n",
      "\n",
      "Studio notebooks, SageMaker On-Demand notebooks, SageMaker Processing, SageMaker Data Wrangler, SageMaker Training, SageMaker Real-Time Inference, and SageMaker Batch Transform regardless of instance family, size, or Region. For example, you can change usage from a CPU instance ml.c5.xlarge running\n",
      "\n",
      "Question: What is SageMaker Ground Truth?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " SageMaker Ground Truth is a fully managed service that helps you build high-quality labeled datasets for machine learning. It provides a simple and intuitive web-based interface to create labeling jobs, manage human labelers, and monitor their work.\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": \"What is SageMaker Ground Truth?\"})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is SageMaker Ground Truth?\n",
      "Assistant:  SageMaker Ground Truth is a fully managed service that helps you build high-quality labeled datasets for machine learning. It provides a simple and intuitive web-based interface to create labeling jobs, manage human labelers, and monitor their work.\n",
      "Follow Up Input: What is SageMaker Distributed Training?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "SageMaker Data Wrangler, Amazon SageMaker Feature Store, Amazon SageMaker Experiments, Amazon SageMaker Debugger, and Amazon SageMaker Model Monitor, can be added to SageMaker Pipelines.\n",
      "\n",
      "Studio notebooks, SageMaker On-Demand notebooks, SageMaker Processing, SageMaker Data Wrangler, SageMaker Training, SageMaker Real-Time Inference, and SageMaker Batch Transform regardless of instance family, size, or Region. For example, you can change usage from a CPU instance ml.c5.xlarge running\n",
      "\n",
      "potential bias during data preparation. From there, you can use SageMaker Data Wrangler’s pre-built transformations to prepare your data. Once your data is prepared, you can build fully automated ML workflows with Amazon SageMaker Pipelines or import that data into Amazon SageMaker Feature Store.\n",
      "\n",
      "are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed\n",
      "\n",
      "Question:  What is the difference between SageMaker Ground Truth and SageMaker Distributed Training?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " SageMaker Ground Truth is a fully managed service that helps you build high-quality labeled data for machine learning (ML) and natural language processing (NLP) applications. SageMaker Distributed Training is a service that allows you to train large-scale deep learning models on Amazon SageMaker.\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": \"What is SageMaker Distributed Training?\"})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is SageMaker Ground Truth?\n",
      "Assistant:  SageMaker Ground Truth is a fully managed service that helps you build high-quality labeled datasets for machine learning. It provides a simple and intuitive web-based interface to create labeling jobs, manage human labelers, and monitor their work.\n",
      "Human: What is SageMaker Distributed Training?\n",
      "Assistant:  SageMaker Ground Truth is a fully managed service that helps you build high-quality labeled data for machine learning (ML) and natural language processing (NLP) applications. SageMaker Distributed Training is a service that allows you to train large-scale deep learning models on Amazon SageMaker.\n",
      "Follow Up Input: What are the two main types of SageMaker Distributed Training??\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "any use case such as natural langue processing, fraud detection, and predictive maintenance. SageMaker Edge Manager is for ML edge developers who want control over their model, including engineering different model features and monitoring models for drift. Any ML edge developer can use SageMaker\n",
      "\n",
      "are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed\n",
      "\n",
      "with SageMaker. You can easily transfer the results of each stage in and out of SageMaker as your business requirements dictate.\n",
      "\n",
      "and endpoints, based on SageMaker pricing. There is no additional charge for using SageMaker JumpStart.\n",
      "\n",
      "Question:  What are the two main types of SageMaker Distributed Training?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " The two main types of SageMaker Distributed Training are 1) Data Parallelism and 2) Model Parallelism.\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": \"What are the two main types of SageMaker Distributed Training??\"})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Step 5. Additional exercises\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://python.langchain.com/en/latest/modules/indexes/document_loaders.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.document_loaders import PDFPlumberLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia as source\n",
    "\n",
    "위키피디아에서 자료 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Amazon Web Services, Inc. (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered, pay-as-you-go basis. Clients will often use this in combination with autoscaling (a process that allows a client to use more', metadata={'title': 'Amazon Web Services', 'summary': 'Amazon Web Services, Inc. (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered, pay-as-you-go basis. Clients will often use this in combination with autoscaling (a process that allows a client to use more computing in times of high application usage, and then scale down to reduce costs when there is less traffic). These cloud computing web services provide various services related to networking, compute, storage, middleware, IoT and other processing capacity, as well as software tools via AWS server farms.  This frees clients from managing, scaling, and patching hardware and operating systems. \\nOne of the foundational services is Amazon Elastic Compute Cloud (EC2), which allows users to have at their disposal a virtual cluster of computers, with extremely high availability, which can be interacted with over the internet via REST APIs, a CLI or the AWS console.  AWS\\'s virtual computers emulate most of the attributes of a real computer, including hardware central processing units (CPUs) and graphics processing units (GPUs) for processing; local/RAM memory; hard-disk/SSD storage; a choice of operating systems; networking; and pre-loaded application software such as web servers, databases, and customer relationship management (CRM).\\nAWS services are delivered to customers via a network of AWS server farms located throughout the world. Fees are based on a combination of usage (known as a \"Pay-as-you-go\" model), hardware, operating system, software, or networking features chosen by the subscriber required availability, redundancy, security, and service options. Subscribers can pay for a single virtual AWS computer, a dedicated physical computer, or clusters of either. Amazon provides select portions of security for subscribers (e.g. physical security of the data centers) while other aspects of security are the responsibility of the subscriber (e.g. account management, vulnerability scanning, patching). AWS operates from many global geographical regions including seven in North America.Amazon markets AWS to subscribers as a way of obtaining large-scale computing capacity more quickly and cheaply than building an actual physical server farm. All services are billed based on usage, but each service measures usage in varying ways. As of 2021 Q4, AWS has 33% market share for cloud infrastructure while the next two competitors Microsoft Azure and Google Cloud have 21%, and 10% respectively, according to Synergy Group.', 'source': 'https://en.wikipedia.org/wiki/Amazon_Web_Services'})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_loader = WikipediaLoader(query=\"AWS\", load_max_docs=2)\n",
    "wikipedia_texts = wikipedia_loader.load_and_split(text_splitter=text_splitter)\n",
    "wikipedia_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs as source\n",
    "\n",
    "인터넷 웹페이지 크롤링 - Amazon Rekognition 온라인 문서 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splitted texts: 124\n",
      "page_content='AWS\\n\\nDocumentation\\n\\nAmazon Rekognition\\n\\nDeveloper Guide\\n\\nLabel Response Objects\\n\\nDetecting objects and concepts\\n\\nThis section provides information for detecting labels in images and videos with Amazon Rekognition Image\\n        and Amazon Rekognition Video.' metadata={'source': 'https://docs.aws.amazon.com/rekognition/latest/dg/labels.html'}\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://docs.aws.amazon.com/rekognition/latest/dg/labels.html\", \n",
    "    \"https://docs.aws.amazon.com/rekognition/latest/dg/faces.html\",\n",
    "    \"https://docs.aws.amazon.com/rekognition/latest/dg/collections.html\",\n",
    "    \"https://docs.aws.amazon.com/rekognition/latest/dg/celebrities.html\"\n",
    "]\n",
    "url_loader = UnstructuredURLLoader(urls=urls)\n",
    "url_texts = url_loader.load_and_split(text_splitter=text_splitter)\n",
    "print(f\"Number of splitted texts: {len(url_texts)}\")\n",
    "print(url_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF source\n",
    "\n",
    "PDF 소스 활용 - RAG 논문 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "external_dataset_folder = f\"{dataset_folder}/external\"\n",
    "os.makedirs(external_dataset_folder, exist_ok=True)\n",
    "\n",
    "sagemaker_pdf_url = \"https://arxiv.org/pdf/2005.11401\"\n",
    "response = requests.get(sagemaker_pdf_url)\n",
    "file = open(f\"{external_dataset_folder}/rag_paper.pdf\", \"wb\")\n",
    "file.write(response.content)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#possible free options: PyPDFLoader, PDFPlumberLoader, PyMuPDFLoader, PDFMinerLoader, PyPDFium2Loader\n",
    "pdf_loader = PDFPlumberLoader(f\"{external_dataset_folder}/rag_paper.pdf\")\n",
    "pdf_texts = pdf_loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vector index\n",
    "\n",
    "위키피디아 + PDF + 웹크롤링 정보로 벡터 인덱스 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total texts: 406\n"
     ]
    }
   ],
   "source": [
    "all_texts = wikipedia_texts + pdf_texts + url_texts\n",
    "print(f\"Number of total texts: {len(all_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 840 ms, sys: 51.3 ms, total: 891 ms\n",
      "Wall time: 47.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Embed your texts\n",
    "agg_vectorstore = FAISS.from_documents(all_texts, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rekognition_question = \"What kind of information does Amazon Rekognition Image returns about image quality?\"\n",
    "aws_question = \"What is AWS market share for cloud infrastructure?\"\n",
    "rag_question = \"What datasets were used for experiments with RAG?\"\n",
    "questions_list = [rekognition_question, aws_question, rag_question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mWhat kind of information does Amazon Rekognition Image returns about image quality?\u001b[0m\n",
      " Amazon Rekognition Image does not provide information about image quality. It only detects objects and labels in images.\n",
      "\n",
      "\n",
      "\u001b[32mWhat is AWS market share for cloud infrastructure?\u001b[0m\n",
      " As of 2021 Q4, AWS has 33% market share for cloud infrastructure while the next two competitors Microsoft Azure and Google Cloud have 21%, and 10% respectively.\n",
      "\n",
      "\n",
      "\u001b[32mWhat datasets were used for experiments with RAG?\u001b[0m\n",
      " The experiments with RAG were conducted using the FEVER dataset, which is a collection of 10,000 claims extracted from Wikipedia and annotated with their verifiability status.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in questions_list:\n",
    "    res_docs = agg_vectorstore.similarity_search(q, k=5)\n",
    "    result = chain({\"input_documents\": res_docs, \"question\": q})\n",
    "    print(colored(q, 'green'))\n",
    "    print(result[\"output_text\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
